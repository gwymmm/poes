\chapter{Anwendungen von Predictive Analytics im öffentlichen Sektor}

Zum Schluss werden einige Anwendungen von Predictive Analytics im öffentlichen
Sektor vorgestellt. Es wurden Anwendungsfälle in den Bereichen öffentliche Verwaltung,
Gesundheit und Bildung betrachtet. Zudem wurden Fallstudien erstellt, bei denen
konkrete Anwendungen im Fokus stehen.

% TODO
% 2 grundlegende Probleme
% - Datenschutz: Achtung Privatsphäre + Angst vor Missbrauch
% - Verzerrungen in Daten: insb. auch Big Data, Daten nicht repräsentativ,
%     Daten kulturabhängig

\section{Öffentliche Verwaltung}


\subsection{Open Data Portale}

Open Data Portale veröffentlichen Daten von Behörden, sodass prinzipiell jeder
sie nutzen kann. So existiert in den USA beispielsweise das Portal \glqq{Data.gov}\grqq{}\footnote{
URL für das US-Portal: \url{https://www.data.gov/}
}, das deutsche Pendant dazu heißt \glqq{GovData.de}\grqq{}\footnote{
URL für die deutsche Version: \url{https://www.govdata.de/} 
} (vgl. \cite{Borchers}). 

Grundsätzlich könnten die Daten auch für Predictive Analytics Zwecke genutzt werden.
Allerdings hat eine stichprobenartige Untersuchung ergeben, dass es sich bei vielen Daten
nicht mehr um \glqq{Rohdaten}\grqq{} handelt, sondern um eine aufbereitete Fassung, die sich
für die visuelle Präsentation eignet. Für Datenanalysen wären jedoch die Rohdaten besser geeignet.
Weiterhin scheinen manche Behörden im Vergleich zu anderen viele Daten an das Portal zu übergeben,
sodass die Auswahl dann unausgeglichen wirkt. Zudem gibt es die Kritik, dass viele relevante
Datensätze nicht veröffentlicht werden (vgl. \cite{Krempl}).

% TODO Daten (-> Heise) 
% Thapa_Parycek S. 46

Insgesamt ist aus der stichprobenartigen Untersuchung der Datensätze auf GovData.de nicht unmittelbar
hervorgegangen, wie die Daten für Predictive Analytics Anwendungen sinnvoll eingesetzt werden könnten. 

\subsection{Predictive Analytics in der Steuerverwaltung}

Die Anwendungen von Predictive Analytics in der Steuerverwaltung werden in einer
Studie des \emph{Forum on Tax Administration} (FTA) diskutiert\footnote{
Deutschland ist bei dieser Studie allerdings nicht dabei.
}. Der folgende Text beschreibt diese Anwendungsfälle.

\subsubsection{Audit Case Selection}

Steuerbehörden führen Kontrollen von Unternehmen und Personen durch, um Steuerbetrug
aufzudecken und zu verringern. Um verfügbare Ressourcen besser einzusetzen, werden mit Hilfe
von Predictive Analytics die Fälle für die Prüfung priorisiert, bei denen der größte Betrugsverdacht
besteht (Risikogruppen). Dieser Prozess wird als \emph{Audit Case Selection} bezeichnet und ist das hauptsächliche
Anwendungsgebiet von Predictive Analytics in der Steuerverwaltung (vgl. \cite{OECD}, S.~20).

Eine Besonderheit dabei ist die Analyse sozialer Netzwerke (\emph{Social Network Analysis}; \cite{OECD}, S.~21-22),
die zur Aufdeckung besonderer Formen von Einkommenssteuerbetrug eingesetzt wird. Die Analyse sozialer Netzwerke kann
Risikogruppen finden, die von Analysen auf individueller Ebene übersehen werden. Bei der Analyse werden die Beziehungen
zwischen Individuen erkennbar und können als visualisierte Netzwerke betrachtet werden. Die weitere Analyse dieser Netzwerke
kann durch Sachbearbeiter erfolgen oder mit Hilfe von regelbasierten oder statistischen Modellen durchgeführt werden.

\subsubsection{Payment Compliance}

Das Ziel der Payment Compliance ist es, ausstehende Zahlungen möglichst einzufordern oder das Problem gar nicht erst
entstehen zu lassen (vgl. \cite{OECD}, S.~24). Die typische Aufgabe von Predictive Analytics ist hierbei, die
Steuerzahler herauszufiltern, bei denen das größte Risiko besteht, dass sie ihre Zahlungsverpflichtungen nicht erfüllen werden. 
Eine weitere Anwendung, ist herauszufinden, wie am effektivsten mit dieser Gruppe kommuniziert werden kann (Prescriptive Analytics).

\subsubsection{Debt Management}

Beim traditionellen Debt Management werden Gruppen von Hochrisikoschuldnern identifiziert, um vorhandene Ressourcen auf sie zu
konzentrieren (vgl. \cite{OECD}, S.~26). Bei einer neueren Methode (\emph{Uplift Modelling}) werden Fälle selektiert, die
mit möglichst hoher Wahrscheinlichkeit auf eine Intervention seitens der Behörde reagieren werden. 

\subsubsection{Verbesserung der Serviceleistungen für Steuerzahler}

Text Mining (inklusive \emph{Sentiment Analysis}) kommt bei den Steuerbehörden zum Einsatz, um die Kommunikation mit den
Steuerzahlern zu verbessern (vgl. \cite{OECD}, S.~27). So werden beispielsweise ankommende E-Mails in Singapur mit Hilfe
von Text Mining bearbeitet, um die Art der Anfrage herauszufinden (vgl. \cite{OECD}, S.~27-28). In einem Fall konnte eine
Gruppe ähnlicher Anfragen identifiziert werden, nachdem eine Steuerrichtlinie verändert worden war. Die Behörde war dann in
der Lage frühzeitig zu reagieren und die Steuerzahler zusätzlich zu informieren. Text Mining hat in Singapur die manuelle
Bearbeitung von E-Mail Anfragen abgelöst. Dies ermöglicht eine objektivere Verarbeitung der Anfragen, da weniger Missverständnisse
entstehen. Zusätzlich wird die Arbeitszeit der Sachbearbeiter eingespart.

\subsubsection{Entscheidungsunterstützung}

Die meisten Datenanalysen bei den Steuerbehörden werden durchgeführt, um operative
Entscheidungen zu unterstützen (vgl. \cite{OECD}, S.~28). Allerdings exisitieren auch
Anwendungen, die strategische und politische Entscheidungen unterstützen. So werden
Analysen zur Abschätzung der Steuerlücke (\emph{tax gap analysis};vgl. \cite{OECD}, S.~28) durchgeführt.
Weiterhin wird versucht, den Einfluss von Änderungen in der Steuerpolitik vorherzusagen. Ein Beispiel
dafür, wenn auch streng genommen keine Predictive Analytics Anwendung, ist das ökonomische Modell, das
2012 von den chinesischen Behörden erstellt wurde, um die Effekte einer Steuerreform auf die Wirtschaft
und die soziale Wohlfahrt abzuschätzen (vgl. \cite{OECD}, S.~29). Der entsprechende Bericht der
Datenanalysten spielte eine wichtige Rolle in dem folgenden Reformprozess.

\subsubsection{Probleme mit Daten}

Der Bericht des FTA enthält auch eine Diskussion einiger Probleme, die im Zusammenhang mit Daten
festgestellt wurden.

So gibt es Bedenken, ob die gesammelten Daten, die anschließend für das Training der Modelle verwendet werden,
repräsentativ sind (vgl. \cite{OECD}, S.~51). Denn die verwendeten Daten stammen oft von stark verzerrten
Stichproben, wie beispielsweise alten Untersuchungsfällen. Somit wird nur ein kleiner Ausschnitt aus der
Gesamtpopulation zum Training der Modelle verwendet, was zu Selektionseffekten und schließlich zu Verzerrungen
im Modell führt. Um dieses Problem zu beheben, beziehen US-Steuerbehörden ihre Daten aus zufällig ausgewählten
Stichproben von Steuerprüfungen, um die Daten repräsentativer zu gestalten (vgl. \cite{OECD}, S.~52).

Weiterhin werden die Daten für Predictive Analytics Projekte eigentlich für operative Zwecke erhoben und
gespeichert (vgl. \cite{OECD}, S.~52). Aus diesem Grund ergeben sich verpasste Gelegenheiten, denn manche Daten, die nicht sinnvoll
für operative Zwecke sind, können nützlich für Datenanalysen sein. Zum Beispiel werden Beanstandungsgründe nach
einer Steuerprüfung typischerweise nicht gespeichert. Diese Information wäre jedoch hilfreich, um gesonderte Modelle
für verschiedene Beanstandungsgründe zu erstellen (vgl. \cite{OECD}, S.~52). 

\subsection{Predictive Policing}

Predictive Policing bezeichnet die Erstellung von Datenanalysen, um Verbrechen zu verhindern, Verbrechensfälle zu lösen
und wahrscheinliche Ziele für Polizeieinsätze zu identifizieren (vgl. \cite{Perry}, S.~1-2). Dabei können diese Methoden
in folgende vier Kategorien eingeteilt werden (vgl. \cite{Perry}, S.~8-9):

\begin{description}

\item[(1) Vorhersage von Verbrechen:] \hfill \\
Bei diesen Methoden sollen Plätze und Zeiträume identifiziert werden,
bei denen das Risiko für Verbrechen besonders hoch ist.

\item[(2) Vorhersage von Kriminellen:] \hfill \\
Hierbei sollen einzelne Personen identifiziert werden, die ein Risiko
tragen, in Zukunft Verbrechen zu begehen.

\item[(3) Vorhersage von Täterprofilen:] \hfill \\
Diese Methoden sollen Täterprofile generieren, die begangene Verbrechen
den wahrscheinlichen Tätern zuordnen.

\item[(4) Vorhersage der Opfer von Kriminalität:] \hfill \\
Hierbei sollen bestimmte Gruppen oder Einzelpersonen identifiziert werden,
bei denen ein erhöhtes Risiko besteht, Opfer von Verbrechen zu werden.

\end{description} 

\begin{figure}%[!hbt]
\centering
\caption{Predictive Policing Prozess}
\includegraphics[scale=1.1]{Grafiken/Predictive_Policing_Ink.pdf} 
\label{pic:Predictive_Policing}
\end{figure}

In Deutschland werden bisher nur die Methoden von Punkt (1) verwendet, mit denen
Delikte wie Wohnungseinbrüche behandelt werden (vgl. \cite{Heuberger}).
Abbildung~\ref{pic:Predictive_Policing} zeigt hierbei das Vorgehen\footnote{
Das Original ist in \cite{Bode}, S.~2 zu finden.
}. Dies ist ähnlich zu den
allgemeinen Vorgehensmodellen aus Abschnitt~\xcom. Insbesondere die anwendungstypischen Details werden
im folgenden Text erläutert:

\begin{description}

\item[(1) Daten:] \hfill \\
Hierbei können polizeiliche und nicht-polizeiliche Daten kombiniert werden. Weiterhin
besteht die Notwendigkeit, die verschiedenen Daten korrekt zusammenzuführen, damit beispielsweise die räumliche
und zeitliche Konsistenz gewährleistet ist (vgl. \cite{Bode}, S.~2). Zudem werden bisher keine
personenbezogenen Daten verwendet (vgl. \cite{Bode}, S.~2 und \cite{Heuberger}) und die Daten konzentrieren
sich auf Wohnungseinbruchdiebstahl (vgl. \cite{Bode}, S.~2)

\item[(2) Modellierung:] \hfill \\
Für die Modellierung können die üblichen Algorithmen wie Regression, Entscheidungsbäume oder
neuronale Netze verwendet werden (vgl. \cite{Bode}, S.~2).

\item[(3) Prognoseberechnung:] \hfill \\
Das Ergebnis der Prognose ist eine Menge an Gebieten, die für den betrachteten Zeitraum
ein höheres Kriminalitätsrisiko aufweisen (vgl. \cite{Bode}, S.~2).

\item[(4) Prognosedarstellung:] \hfill \\
Neben der altmodischen Variante von Karten im Papierformat, existieren auch graphische
Visualisierungen mit Hilfe von Tablet-PCs oder Mobilfunkgeräten (vgl. \cite{Bode}, S.~3).

\item[(5) Prognoseverwertung:] \hfill \\
Die Prognosen werden durch operative Polizeieinheiten genutzt, die ihre Aktionen mit Hilfe
der Vorhersagen planen können (vgl. \cite{Bode}, S.~2).

\item[(6) Evaluation:] \hfill \\
Während das System arbeitet erfolgt auch immer wieder eine Bewertung der Leistung des Systems.
Die Evaluation beeinflusst die Schritte (1)-(5) (vgl. \cite{Bode}, S.~3).

\end{description}

\subsubsection{Bisheriges Fazit von Predictive Policing in Deutschland}

Bereits im RAND Bericht von 2013 wurde betont, dass Computer nicht von alleine die Probleme
lösen können. Es braucht viel Arbeit und Korrektureingriffe von Menschen (vgl. \cite{Perry}, S.~117-118). 
Dies ist sowohl auf datenanalytischer Ebene der Fall, als auch auf der sozialen Ebene, auf der
\glqq{Feldarbeit}\grqq{} notwendig ist, um das Wissen über die konkrete Kriminalitätsumgebung auf dem
neuesten Stand zu halten. 

Das Fazit von Predictive Policing in Deutschland ist bisher gemischt. Es gibt Berichte über hohe
Trefferquoten der Anwendungen. Diese sollten allerdings mit Skepsis beurteilt
werden, da bei der Berechnung der Trefferquoten mangels konkreter Regelungen Abweichungen
entstehen können (vgl. \cite{Bode}, S.~9).
Diese Unregelmäßigkeiten können in folgenden Bereichen entstehen (vgl. \cite{Bode}, S.~9-11):

\begin{description}
\item[Prognose-Delikt:] Welches Art des Delikts wurde konkret vorhergesagt?
\item[Prognose-Dauer:] Für welchen Zeitraum wurde die Prognose angefertigt?
\item[Prognose-Raum:] Zählen Treffer an Gebietsrändern?
\end{description}

Weiterhin kam ein Evaluationsbericht für die Anwendung des Predictive Policing Werkzeugs PRECOBS
zu dem Schluss, dass es schwer zu beurteilen ist, ob die Anwendung zu einer Verminderung von
Wohnungseinbrüchen geführt hat, und dass das Werkzeug wahrscheinlich nur zu einer moderaten Kriminalitätsminderung
beigetragen hat (vgl. \cite{Gerstner}, S.~85).  

\subsection{Wahlen und Politik}

Neben den in Abschnitt~\xcom erwähnten Text Mining Anwendungen zur Verbesserung der Serviceleistungen
von Behörden existieren noch weitere \glqq{experimentelle}\grqq{} Anwendungen. So konnte Matter durch
die Analyse teilstrukturierter Internetdokumente herausfinden, wie stark öffentliche Ämter in den USA
mit religiösen Personen besetzt sind (vgl. \cite{Matter}). Der \glqq{Bible Belt}\grqq{}\footnote{
Der \glqq{Bible Belt} ist eine besonders religiöse Region der USA, die sich über mehrere Bundesstaaten erstreckt.
} und der Mormonenstaat Utah waren deutlich als relgiös konservative Staaten erkennbar (vgl. \cite{Matter}, S.~1078).
Weiterhin konnten Falck et al. die weltanschauliche Nähe von Zeitungen zu politischen Parteien und einzelnen Politikern
darstellen, indem sie die Einstellungen der Zeitschriften zu den jeweiligen Personen und Parteien analysierten
(\emph{sentiment analysis}; vgl. \cite{Falck}, S.~4). Somit können mit Hilfe von Text Mining die weltanschaulichen Präferenzen
von Personen und Organisationen bestimmt werden. Allerdings ist der praktische Nutzen solcher Analysen (noch) nicht klar.
Personen, die sich mit Politik beschäftigen, können solche weltanschaulichen Differenzen mit Hilfe ihrer Erfahrung relativ
schnell einschätzen.

In den USA werden Datenanalysen bei Wahlkämpfen verwendet. So waren im Wahlkampfteam von
Hillary Clinton im Jahr 2016 mehr als 60 Mathematiker und Datenanalysten beschäftigt (vgl. \cite{Goldmacher}). 
Die Modelle, die sie anwenden, dienen unter anderem dazu, \glqq{Wackelkandidaten}\grqq{} unter Delegierten zu identifizieren,
um die verfügbaren Ressourcen an Wahlwerbung auf sie zu fokussieren (vgl. \cite{Goldmacher}). Allerdings hat Donald Trump bei der damaligen Wahl
fast nichts in Datenanalysen investiert (vgl. \cite{Goldmacher}) und trotzdem gewonnen.

Ein bekannteres (und umstritteneres) Beispiel für die Anwendung von Datenanalysen
für politische und wahlkampftaktische Ziele ist der Fall von Cambridge Analytica.

\subsubsection{Cambridge Analytica - Fallstudie}

\begin{figure}%[!hbt]
\centering
\caption{Nutzung von Facebook Likes zur Vorhersage von Nutzerattributen}
\includegraphics[scale=1.0]{Grafiken/Facebook_Likes_Ink.pdf} 
\label{pic:Like_Matrix}
\end{figure}

Den Grundstein für die Tätigkeit von \emph{Cambridge Analytica} hat ein Artikel
von Forschern der Cambridge Universität gelegt\footnote{
Genauer: Forscher vom \emph{Psychometrics Centre} in Cambridge
} (\cite{Kosinski}). Diese haben Persönlichkeitsmerkmale und Eigenschaften von
Personen vorhergesagt, indem sie die Nutzung von \emph{Facebook Likes} analysierten.
Abbildung~\ref{pic:Like_Matrix} veranschaulicht dieses Vorgehen\footnote{
Für die Orignalgrafik siehe \cite{Kosinski}, S.~5803.
}. Die Facebook Likes (Variablen) von tausenden Nutzern wurden mit Hilfe einer Methode zur
Variablenreduktion\footnote{
Die Singulärwertzerlegung ist eine Methode, die ähnlich zur Principal Component Analysis ist.
} auf 100 Komponenten verkleinert. Diese Komponenten dienten anschließend als Prädiktorvariablen
für die Vorhersagemodelle. Das Alter wurde beispielsweise mit linearer Regression bestimmt.
Die Modell konnte mit Hilfe der Informationen aus dem Facebook Profil, dem
Profilbild und der Ergebnisse eines psychologischen Tests trainiert werden (Trainingsdatensatz).
Das Ergebnis der Studie war, dass die Persönlichkeitsmerkmale und Attribute von Personen mit hoher
Genauigkeit von den Modellen vorhergesagt werden konnten\footnote{
Manche Attribute,wie z. B. das Alter, konnten genauer vorhergesagt werden als andere Merkmale, wie z. B. die Lebenszufriedenheit (vgl. \cite{Kosinski}, S.~5804)
} (vgl. \cite{Kosinski}, S.~5804).

Es wird vermutet, dass die Forscher aus Cambridge oder die Universität selbst sich geweigert haben, diese Nutzerdaten
preiszugeben (vgl. \cite{Isaak}, S.~57). Eine andere Forschergruppe hat in Kooperation mit Cambridge Analytica die
erforderlichen Daten mit Hilfe einer Umfrage von Facebook Nutzern gesammelt und dabei durch die Freigiebigkeit der Schnittstelle von
Facebook auch gleich die Daten der Freunde der Befragten erhalten (vgl. \cite{Isaak}, S.~57).

Das Ziel von Cambridge Analytica war es, mit dem Prognosemodell bestimmte Wählergruppen zu identifizieren (vgl. \cite{Isaak}, S.~57).
Diese würden dann im Laufe von Wahlen gezielt Nachrichten erhalten (Mikrotargeting). Dabei sollten entweder Wähler davon abgehalten
werden für den Konkurrenten zu stimmen, oder Wähler dazu verleitet werden für den gewünschten Kandidaten zu stimmen.

Die Einschätzungen über die Effektivität solcher Mikrotargeting Methoden gehen weit auseinander. So schreibt Trump in einem
Artikel der \emph{Washington Post} unter anderem, dass es sehr schwer sei mit Hilfe vom Mikrotargeting die politischen Ansichten
einer Person zu beeinflussen (vgl. \cite{Trump}). Außerdem hätten andere Mikrotargeting Kampagnen in der Vergangenheit keinen
eindeutigen Nutzen gezeigt. Andere Stimmen behaupten, dass die Arbeit von Cambridge Analytica einen nützlichen oder sogar
kritischen Effekt auf den Wahlausgang von 2016 hatte (vgl. \cite{Isaak}, S.~58).

Christliche Vereinigungen in den USA nutzen ähnliche Methoden, mit dem Ziel neue Mitglieder für die Kirche
zu gewinnen (vgl. \cite{Viken}, Minuten 13-19). So ist beispielsweise \emph{Gloo} eine christliche Softwarefirma,
die sich auf die Verknüpfung von Datenbanken spezialisiert hat. Gloo vermarktet seine Mikrotargeting Plattform
\emph{Insights} an Kirchen, wobei auch Daten der Kirchenmitglieder erfasst werden. Ein Werbespot für Insights
wirbt damit, ihren Kunden eine \glqq{Rundumansicht ihrer Zielgruppe in einer geschützten Datenbank mit mehr als 
2200 Datenpunkten zu nahezu jedem US Verbraucher}\grqq{} (siehe \cite{Viken}) zu bieten. Matt Engel, ein Gloo
Mitarbeiter, meint dass Gloo die größte Datenplattform mit missionarischem Ziel weltweit aufgebaut habe (vgl. \cite{Viken}).

Ein Geldgeber und Untestützer von Gloo beschreibt in einem Szenario, wie die Methode von Insights funktioniert (vgl. \cite{Viken}).
Die Anwendung identifiziert eine Zielgruppe von etwa 5 000 bis 10 000 Personen in einem Umkreis von etwa 15~km um eine Kirche.
Aus den Daten zu den Einkäufen der Personen kann geschlossen werden, ob manche Paare vielleicht Eheprobleme haben.
Diese Personen werden dann ausgewählt und die Anwendung hilft der Gemeinde, Kontakt mit ihnen über Facebook herzustellen.
Von der Kirche werden dann gezielt Einladungen zu Treffen, wie beispielsweise Abendveranstaltungen, verschickt. Wenn das Ehepaar
Kinder hat, werden sie während der Veranstaltung kostenlos betreut. Die neuen Kirchenbesucher können dann Kontakte mit
Mitgliedern aus der Kirchengemeinde knüpfen und möglicherweise selbst beitreten oder sich in Zukunft stärker engagieren.
Wie bereits oben erwähnt, dient die Methode also dazu, alte Mitglieder zu halten und neue für die Kirche zu gewinnen,
und somit die Kirchengemeinde zu vergrößern.

Einschätzungen dazu, wie effektiv das christliche Mikrotargeting funktioniert, wurden allerdings nicht gegeben.

\section{Gesundheit}

Schon seit dem letzten Jahrhundert können statistische Methoden von Wissenschaftlern
genutzt werden, um Gefahren für die öffentliche Gesundheit zu untersuchen (vgl. \cite{Proctor}, S.~216).

Allerdings hat sich dadurch auch das Phänomen der \glqq{Evidenzbeharrlichkeit}\grqq{} in der Epidemiologie etabliert
(vgl. \cite{Proctor}, S.~112-113).
Epidemiologen beharren auf strenger Evidenz und großen Fallzahlen (vgl. \cite{Proctor}, S.~112-113) und ignorieren weniger 
starke Indizien, wodurch der Fortschritt manchmal blockiert wird. Dies war bei der Anerkennung der Gesundheitsgefahren
von Asbest der Fall (vgl. \cite{Proctor}, S.~112-113). Im Jahr 1938 lag die Zahl der bekannten Krebserkankungen im Zusammenhang mit
Asbest weltweit bei 6. Somit konnten Epidemiologen behaupten, dass die Fallzahl nicht reiche, um einen signifikanten Zusammenhang
zwischen Asbest und Lungenkrebs herzustellen. Die Beweiskraft einer klinischen Studie, die den Zusammenhang hergestellt hat, indem sie
zwei Einzelfälle untersucht hat, galt als zu schwach (vgl. \cite{Proctor}, S.~113). Somit wurde die krebserregende Wirkung von Asbest
in Großbritannien und den Vereinigten Staaten nicht in den frühen 1940er Jahren sondern mehr als 20 Jahre danach anerkannt.

Die Geschichte scheint sich während der COVID-19 Pandemie wiederholt zu haben. Es wurde damit gehadert, Ergebnisse von Studien anzuerkennen,
die behaupteten, dass einfache Masken das Infektionsrisiko senken, weil sie nicht die strengen Kriterien der evidenzbasierten Medizin
entsprachen (vgl. \cite{Jotten}). Somit wurde das Tragen von Masken anfangs nicht vom Robert-Koch-Insitut empfohlen.

\subsection{Datenschutzbedenken}
% TODO Datenquellen

Das Thema Datenschutz bestimmt in Deutschland die Diskussion von Datenanalysen
im Gesundheitssektor (vgl. \cite{Jorzig}, S.~138-139 und \cite{Weichert}, S.~835-836 und \cite{Moebus}, S.~903). 
Insbesondere der Grundsatz der Datensparsamkeit, wozu Datenvermeidung, Datenlöschung und das Prinzip der Nichtverkettbarkeit gehören,
steht im Gegensatz zu den Bedürfnissen des Big Data Konzepts, das durch Anhäufung und Verknüpfung vieler Daten Erkenntnisgewinne
sucht (vgl. \cite{Weichert}, S.~836). Aus diesem Grund müssen Gesundheitsdaten entweder annonymisiert oder pseudonymisiert werden:

\begin{description}

\item[Anonymisierung:] \hfill \\
Bei der Anonymisierung der Daten darf die Reidentifizierung nicht mehr erfolgen können.
Somit müssen bei Patientendaten nicht nur die Identifikationsmerkmale der Patienten entfernt werden, sondern
auch die Identifikatoren sämtlicher Dienstleister, die eine Zuordnung des Patienten vornehmen könnten,
entfernt werden (vgl. \cite{Weichert}, S.~836). Zu solchen Dienstleistern zählen beipielsweise Ärzte oder Apotheker.
Zusätzlich muss für eine vollständige Anonymisierung eine Aggregierung der Daten erfolgen, wobei z. B. Individualdatensätze
zu Gruppendatensätzen zusammengefasst werden, was allerdings die Datenqualität senkt (vgl. \cite{Weichert}, S.~836).

\item[Pseudonymisierung:] \hfill \\
Eine weitere Möglichkeit, die Daten datenschutzrechtlich nutzbar zu machen, ist die Pseudonymisierung.
Dabei werden die Identifikationsmerkmale von Patienten durch Kennzeichen ersetzt, was die Wiederzuordnung der Daten
erschweren soll (vgl. \cite{Weichert}, S.~836). Die Auswertung der pseudonymisierten Daten muss dann in einer geschützen
Umgebung erfolgen, die die Wiederzuordnung der Daten ausschließen soll (vgl. \cite{Weichert}, S.~837).
Dieser Ansatz wird auch als \emph{Differential Privacy} bezeichnet (vgl. \cite{Jorzig}, S.~140-141).

\end{description}

Hierbei sei jedoch erwähnt, dass die Datenschutzgrundverordnung, Datenanalysen zu wissenschaftlichen Zwecken erlaubt
(vgl. \cite{Jorzig}, S.~139). Dies wird als Forschungsprivileg bezeichnet. Allerdings sind müssen die oben genannten
strengen Einschränkungen beachtet werden.

\subsection{Predictive Analytics in der Radiologie}

Das Fachgebiet der Radiologie nimmt im Bereich Datenanalysene eine Vorreiterstellung in Deutschland ein (vgl. \cite{Jorzig}, S.~111-112).
Dabei muss die Radiologie als erste medizinische Fachrichtung den Kontakt mit den Behörden suchen, das der Einsatz
dieser Systeme in Deutschland bisher nicht reguliert ist (vgl. \cite{Jorzig}, S.~112).

Diese Sonderposition der Radiologie hängt unter anderem mit dem Problem von \glqq{Zufallsbefunden}\grqq{} zusammen.
Wenn ein Radiologe sich eine Aufnahme eines Patienten ansieht und neben dem Hauptbefund (sein \glqq{Hauptziel}\grqq{})
ein weiterer Befund (Nebenbefund) vorliegt, den der Radiologe nicht erkennt, so muss der Radiologe für diesen Fehler haften
(vgl. \cite{Jorzig}, S.~112). Diesem Haftungsdruck ausgesetzt, muss die Radiologie sich modernisieren. Ein System,
das Röntgen oder CT-Aufnahmen\footnote{Computertomographie Aufnahmen} analysieren und Befunde prognostizieren kann, kann
bei jeder Kontrolle die Aufnahme auf Nebenbefunde untersuchen (vgl. \cite{Jorzig}, S.~112).
Damit wird der Radiologe aus der Haftung genommen. Ein Beispiel für ein solches System ist
PANTHER\footnote{Patientenorientierte onkologische Therapieassistenz} vom Fraunhofer Institute for Medical Image in Bremen.
Das System wird mit CT-Bildern als Daten trainiert, wobei auf den Trainingsbildern Tumorgewebe und gutartiges Gewebe markiert sind
(vgl. \cite{Jorzig}, S.~112).

Am Beispiel der Radiologie lässt sich erkennen, wie ein Fachgebiet durch äußeren Druck (Haftungsklagen) zur Anwendung von Predictive
Analytics gedrängt wird. Für den Radiologen können solche Systeme eine Gefahr für seine Arbeit darstellen, wenn sein Tätigkeitsbereich
zunehmend automatisiert wird. Allerdings ändert der externe Druck die \glqq{Spielregeln}\grqq{} (siehe S~\xcom), sodass
sich Predictive Analytics zunehmend etabliert.

% TODO Missbrauch -> Proctor
\subsection{Seuchenerkennung mit AI}

% TODO
% Google Flu Trends (vorerst gescheitert)

% TODO
%\subsection{Flint Trinkwasserskandal - Fallstudie}

\section{Bildung}
Abschließend sollen noch die Anwendungsmöglichkeiten von Predictive Analytics
im Bildungssektor kurz skizziert werden.

Eine weitere Anwendung ist E$^2$Coach, ein System zur Studienunterstützung.

\subsection{University of Michigan E$^2$Coach - Fallstudie}

E$^2$Coach wurde an der Universität von Michigan entwickelt (vgl. \cite{Mattingly}, S.~242).
Der Zweck der Einführung von E$^2$Coach war eine Verringerung der Abbrecherquoten in den
MINT-Fächern zu erreichen. Als historische Daten dienten dabei die Studienverlaufsdaten von
49 000 Studienanfängern im Fach Physik. Aus diesen Daten wurde ein Modell generiert.
Risikostudenten können mit Hilfe dieses Modells identifiziert werden, indem ihre Daten in das
System eingegeben werden. Dabei liefern Tutoren, Physikprofessoren und die Studenten selbst
die individuellen Daten der Studenten.

Um Feedback für die Studenten zu ermöglichen, agiert E$^2$Coach als Schnittstelle zwischen
den Studenten und den Ressourcen, die ihnen zur Verfügung stehen (z. B. Online Lernmaterial) (vgl. \cite{Mattingly}, S.~242).
So bietet das System den Studenten Fortschrittsanzeigen, Vorschläge für Lernmethoden und Ermutigungen.
Zudem gibt es menschliche Berater, die mit den Studenten über eine Online-Plattform kommunizieren können.
Bei den Beratern handelt es sich um Tutoren, die bereits einen Studienabschluss besitzen und die auf Basis
gemeinsamer Ziele und Interessen den zu beratenden Studenten zugewiesen werden.

Anwendungen wie E$^2$Coach, wären womöglich auch in Deutschland denkbar, falls die Nutzung personenbezogener
Daten auf Freiwilligkeit basiert.

Insgesamt scheint der Bildungssektor bisher am wenigsten von
Predictive Analytics Anwendungen profitiert zu haben.

% konkreteres Anwendungsbeispiel Decision Tree
